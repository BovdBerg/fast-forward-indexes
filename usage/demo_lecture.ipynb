{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce TCT-ColBERT FF Interpolation\n",
    "\n",
    "_IN5000, Master Thesis, group: Web Information Systems, theme: Information Retrieval, TU Delft_\n",
    "\n",
    "_Bo van den Berg, b.vandenberg-6@student.tudelft.nl_\n",
    "\n",
    "- https://github.com/wis-delft/in4325-information-retrieval\n",
    "- https://colab.research.google.com/github/wis-delft/in4325-information-retrieval/blob/main/intro-pyterrier/07-neural_models.ipynb#scrollTo=NdAb9cjkPlTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting GPU for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: In order to train a large neural network in reasonable time, you'll need a CUDA-capable GPU. \n",
    "If you have one, follow the [official tutorials](https://pytorch.org/get-started/locally/) and install PyTorch with CUDA acceleration. \n",
    "\n",
    "If you do not have one, Google Colab offers free GPUs and TPUs. \n",
    "Please do the following: \n",
    "\n",
    "`Edit -> Notebook settings -> Hardware accelerator -> select a GPU`\n",
    "\n",
    "If the installation was successful, restart your kernel. \n",
    "Then run the following cell to confirm that the GPU is detected. \n",
    "Now the following should no longer return `using the CPU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Quadro P1000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available, use it.\n",
    "if torch.cuda.is_available():\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(tqdm=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Side note_: In this notebook, we focus on the **retrieve-and-re-rank** setting. PyTerrier supports **dense retrieval** models through plugins (such as [pyterrier_ance](https://github.com/terrierteam/pyterrier_ance)). Since dense retrieval is often very resource-demanding, we do not cover it here. Another library that provides many pre-trained models and dense retrieval indexes is [pyserini](https://github.com/castorini/pyserini).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast-Forward Indexes\n",
    "\n",
    "Fast-forward indexes use _dual-encoder models_ (the same that are used in dense retrieval) for _interpolation-based re-ranking_. The benefit of this (compared to cross-encoders) is that document representations only need to be computed once (during the indexing step) and can be looked up during re-ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoders\n",
    "\n",
    "We'll start by instantiating the encoders. [TCT-ColBERT](https://github.com/castorini/tct_colbert) is a single-vector dual-encoder model based on BERT, where the query and document encoders are identical (Siamese architecure). A pre-trained model (trained on MS MARCO) is [available on the Hugging Face hub](https://huggingface.co/castorini/tct_colbert-msmarco). We'll use this model in a transfer setting (i.e., without fine-tuning) on the FiQA dataset.\n",
    "\n",
    "The encoders can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.encoder.transformer import TCTColBERTQueryEncoder\n",
    "\n",
    "q_encoder = TCTColBERTQueryEncoder(\"castorini/tct_colbert-msmarco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The index\n",
    "\n",
    "For the dense vector representations, we'll need another separate index.\n",
    "\n",
    "In our case, we can load the provided index instead of indexing everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bovdberg/miniconda3/envs/thesis/lib/python3.10/site-packages/fast_forward/ranking.py:298: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "from fast_forward import Ranking\n",
    "from pathlib import Path\n",
    "\n",
    "k_s = 1000\n",
    "dataset = ir_datasets.load(\"msmarco-passage/trec-dl-2019/judged\")\n",
    "\n",
    "# load a run (TREC format) and attach all required queries\n",
    "ranking = Ranking.from_file(\n",
    "    Path(\"msmarco-passage-test2019-sparse10000.txt\"),\n",
    "    {q.query_id: q.text for q in dataset.queries_iter()},\n",
    ").cut(k_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always load the index on disk instead of indexing everything.\n",
    "\n",
    "We set `dim=768`, because our encoders output `768`-dimensional representations. `Mode.MAXP` determines how documents that have multiple vectors are scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8841823/8841823 [00:14<00:00, 603458.84it/s] \n"
     ]
    }
   ],
   "source": [
    "from fast_forward import OnDiskIndex, Mode\n",
    "from pathlib import Path\n",
    "\n",
    "ff_index = OnDiskIndex.load(\n",
    "    Path(\"../ff_msmarco-v1-passage.tct_colbert.h5\"), \n",
    "    # dim=768,\n",
    "    query_encoder=q_encoder, \n",
    "    mode=Mode.MAXP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, if you have enough RAM, you can load the entire index (i.e., all vector representations) into the main memory:\n",
    "\n",
    "- I don't have enough RAM for msmarco_passage though: \n",
    "`MemoryError: Unable to allocate 25.3 GiB for an array with shape (8841823, 768) and data type float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment next line if the dataset is too large\n",
    "# ff_index = ff_index.to_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ranking\n",
    "\n",
    "In order to use a Fast-Forward index for re-ranking, we wrap it in an `FFScore` transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard re-ranking\n",
    "# ff_out = ff_index(ranking.cut(k_s))\n",
    "ff_out = ff_index(ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.util.pyterrier import FFScore\n",
    "\n",
    "ff_score = FFScore(ff_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `score` column has now been updated to reflect the re-ranking scores. Furthermore, there is a new column, `score_0`, which contains the original retrieval scores. As mentioned earlier, Fast-Forward indexes focus on _interpolation-based re-ranking_. In essence, the idea is to take both lexical retrieval scores $s_{\\text{lex}}$ and semantic re-ranking scores $s_{\\text{sem}}$ into account, such that the final score $s$ is computed as follows:\n",
    "\n",
    "$$s = \\alpha s_{\\text{lex}} + (1-\\alpha) s_{\\text{sem}}$$\n",
    "\n",
    "We can perform the interpolation using the `FFInterpolate` transformer. \n",
    "We'll set the hyperparameter $\\alpha$ to an abritrarily chosen 0.5 for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.util.pyterrier import FFInterpolate\n",
    "\n",
    "ff_int = FFInterpolate(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>AP@1000</th>\n",
       "      <th>R@1000</th>\n",
       "      <th>nDCG@20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCT-ColBERT</td>\n",
       "      <td>0.377308</td>\n",
       "      <td>0.738937</td>\n",
       "      <td>0.491352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCT-ColBERT &gt;&gt; FF</td>\n",
       "      <td>0.455415</td>\n",
       "      <td>0.738937</td>\n",
       "      <td>0.666728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name   AP@1000    R@1000   nDCG@20\n",
       "0        TCT-ColBERT  0.377308  0.738937  0.491352\n",
       "1  TCT-ColBERT >> FF  0.455415  0.738937  0.666728"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier.measures import AP, R, nDCG\n",
    "from fast_forward.util import to_ir_measures\n",
    "\n",
    "dataset = pt.get_dataset('irds:msmarco-passage/trec-dl-2019/judged')\n",
    "\n",
    "pt.Experiment(\n",
    "    [to_ir_measures(ranking), to_ir_measures(ff_out)],\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[AP @ 1000, R @ 1000, nDCG @ 20],\n",
    "    names=[\"TCT-ColBERT\", \"TCT-ColBERT >> FF\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTerrier offers several functions to determine the best hyperparameters for a ranker. In the following, we'll use [`pyterrier.GridSearch`](https://pyterrier.readthedocs.io/en/latest/tuning.html#pyterrier.GridSearch) to find the best value for $\\alpha$.\n",
    "\n",
    "**Important**: When you tune hyperparameters of your model, **do not use the same data you use for testing (i.e., the testset)**. Otherwise, your results are invalid, because you optimized your method for the testing data. Instead, we'll use the development (validation) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devset = pt.get_dataset('irds:msmarco-passage/dev/judged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTerriers `GridSearch` class can be used to automatically run an experiment multiple times in order to find the hyperparameters that result in the best performance.\n",
    "\n",
    "Conveniently, it also sets the best value for us in the transformer.\n",
    "\n",
    "The value of hyperparameters such as $\\alpha$ can make a big difference.\n",
    "\n",
    "We'll use a similar pipeline as before, but we limit the number of candidate documents to `100` in order to reduce the runtime. We provide a list of values for `alpha` and a metric (MAP), which is used to decide which value results in the best performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Please confirm you agree to the MSMARCO data usage agreement found at <http://www.msmarco.org/dataset.aspx>\n",
      "[INFO] [starting] https://msmarco.z22.web.core.windows.net/msmarcoranking/qrels.dev.tsv\n",
      "[INFO] [finished] https://msmarco.z22.web.core.windows.net/msmarcoranking/qrels.dev.tsv: [00:02] [1.20MB] [550kB/s]\n",
      "[INFO] If you have a local copy of https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz, you can symlink it here to avoid downloading it again: /home/bovdberg/.ir_datasets/downloads/c177b2795d5f2dcc524cf00fcd973be1\n",
      "[INFO] [starting] https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz\n",
      "[INFO] [finished] https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz: [00:41] [18.9MB] [458kB/s]\n",
      "                                                                                                  \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9dca8cf25047a8b385c2b1bf746a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridScan:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best map is 0.000000\n",
      "Best setting is ['<fast_forward.util.pyterrier.FFInterpolate object at 0x7ff1987eb970> alpha=0.05']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>962179</td>\n",
       "      <td>8785371</td>\n",
       "      <td>70.537880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>962179</td>\n",
       "      <td>5653659</td>\n",
       "      <td>70.318794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>962179</td>\n",
       "      <td>2329699</td>\n",
       "      <td>70.294601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>962179</td>\n",
       "      <td>2978866</td>\n",
       "      <td>70.273598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>962179</td>\n",
       "      <td>6898289</td>\n",
       "      <td>70.240326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42995</th>\n",
       "      <td>1037798</td>\n",
       "      <td>7783409</td>\n",
       "      <td>63.979805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42996</th>\n",
       "      <td>1037798</td>\n",
       "      <td>4547385</td>\n",
       "      <td>63.891666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42997</th>\n",
       "      <td>1037798</td>\n",
       "      <td>3850121</td>\n",
       "      <td>63.888672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42998</th>\n",
       "      <td>1037798</td>\n",
       "      <td>5538665</td>\n",
       "      <td>63.815487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42999</th>\n",
       "      <td>1037798</td>\n",
       "      <td>5538663</td>\n",
       "      <td>63.671410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query_id   doc_id      score\n",
       "0       962179  8785371  70.537880\n",
       "1       962179  5653659  70.318794\n",
       "2       962179  2329699  70.294601\n",
       "3       962179  2978866  70.273598\n",
       "4       962179  6898289  70.240326\n",
       "...        ...      ...        ...\n",
       "42995  1037798  7783409  63.979805\n",
       "42996  1037798  4547385  63.891666\n",
       "42997  1037798  3850121  63.888672\n",
       "42998  1037798  5538665  63.815487\n",
       "42999  1037798  5538663  63.671410\n",
       "\n",
       "[43000 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pt.GridSearch(\n",
    "#     to_ir_measures(ff_index(ranking)), # ~bm25 % 100 >> ff_score >> ff_int,\n",
    "#     {ff_int: {\"alpha\": [0.05, 0.1, 0.5, 0.9]}},\n",
    "#     devset.get_topics(),\n",
    "#     devset.get_qrels(),\n",
    "#     \"map\",\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Side note_: As of now, PyTerrier does not support caching for re-ranking transformers. Hence, `GridSearch` takes a long time, because the scores are re-computed every time, even though that wouldn't be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>AP@1000</th>\n",
       "      <th>R@1000</th>\n",
       "      <th>nDCG@20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCT-ColBERT</td>\n",
       "      <td>0.377308</td>\n",
       "      <td>0.738937</td>\n",
       "      <td>0.491352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCT-ColBERT &gt;&gt; FF</td>\n",
       "      <td>0.455415</td>\n",
       "      <td>0.738937</td>\n",
       "      <td>0.666728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name   AP@1000    R@1000   nDCG@20\n",
       "0        TCT-ColBERT  0.377308  0.738937  0.491352\n",
       "1  TCT-ColBERT >> FF  0.455415  0.738937  0.666728"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyterrier.measures import AP, R, nDCG\n",
    "# from fast_forward.util import to_ir_measures\n",
    "\n",
    "# dataset = pt.get_dataset('irds:msmarco-passage/trec-dl-2019/judged')\n",
    "\n",
    "# pt.Experiment(\n",
    "#     [to_ir_measures(ranking), to_ir_measures(ff_out)],\n",
    "#     dataset.get_topics(),\n",
    "#     dataset.get_qrels(),\n",
    "#     eval_metrics=[AP @ 1000, R @ 1000, nDCG @ 20],\n",
    "#     names=[\"TCT-ColBERT\", \"TCT-ColBERT >> FF\"],\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
